{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=100\n",
    "training_data = datasets.FashionMNIST(root=\"../fashion_mnist\", train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.FashionMNIST(root=\"../fashion_mnist\", train=False, download=True, transform=transforms.ToTensor())\n",
    "train_dataloader = DataLoader(training_data, batch_size=batchsize, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "num_epochs = 40\n",
    "learning_rate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_len, hidden_size, num_class, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_len, hidden_size, num_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
    "        cell_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(X, (hidden_states, cell_states))\n",
    "        out = self.output_layer(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(28, 128, batch_first=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(input_size, hidden_size, num_classes, num_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, train_dataloader, loss_func, optimizer):\n",
    "    total_steps = len(train_dataloader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (images, labels) in enumerate(train_dataloader):\n",
    "            images = images.reshape(-1, sequence_len, input_size)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch+1) % 100 == 0:\n",
    "                print(f\"Epoch: {epoch+1}; Batch {batch+1}; Loss: {loss.item():>4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Batch 100; Loss: 1.579117\n",
      "Epoch: 1; Batch 200; Loss: 1.158141\n",
      "Epoch: 1; Batch 300; Loss: 0.998229\n",
      "Epoch: 1; Batch 400; Loss: 0.707178\n",
      "Epoch: 1; Batch 500; Loss: 0.999379\n",
      "Epoch: 1; Batch 600; Loss: 0.547712\n",
      "Epoch: 2; Batch 100; Loss: 0.640578\n",
      "Epoch: 2; Batch 200; Loss: 0.548974\n",
      "Epoch: 2; Batch 300; Loss: 0.671081\n",
      "Epoch: 2; Batch 400; Loss: 0.549144\n",
      "Epoch: 2; Batch 500; Loss: 0.503040\n",
      "Epoch: 2; Batch 600; Loss: 0.526565\n",
      "Epoch: 3; Batch 100; Loss: 0.545343\n",
      "Epoch: 3; Batch 200; Loss: 0.326592\n",
      "Epoch: 3; Batch 300; Loss: 0.528337\n",
      "Epoch: 3; Batch 400; Loss: 0.506032\n",
      "Epoch: 3; Batch 500; Loss: 0.339112\n",
      "Epoch: 3; Batch 600; Loss: 0.421843\n",
      "Epoch: 4; Batch 100; Loss: 0.476633\n",
      "Epoch: 4; Batch 200; Loss: 0.398984\n",
      "Epoch: 4; Batch 300; Loss: 0.298647\n",
      "Epoch: 4; Batch 400; Loss: 0.466965\n",
      "Epoch: 4; Batch 500; Loss: 0.515151\n",
      "Epoch: 4; Batch 600; Loss: 0.567961\n",
      "Epoch: 5; Batch 100; Loss: 0.449981\n",
      "Epoch: 5; Batch 200; Loss: 0.402489\n",
      "Epoch: 5; Batch 300; Loss: 0.382654\n",
      "Epoch: 5; Batch 400; Loss: 0.330798\n",
      "Epoch: 5; Batch 500; Loss: 0.423018\n",
      "Epoch: 5; Batch 600; Loss: 0.266721\n",
      "Epoch: 6; Batch 100; Loss: 0.409658\n",
      "Epoch: 6; Batch 200; Loss: 0.285079\n",
      "Epoch: 6; Batch 300; Loss: 0.435282\n",
      "Epoch: 6; Batch 400; Loss: 0.404934\n",
      "Epoch: 6; Batch 500; Loss: 0.331794\n",
      "Epoch: 6; Batch 600; Loss: 0.496110\n",
      "Epoch: 7; Batch 100; Loss: 0.311671\n",
      "Epoch: 7; Batch 200; Loss: 0.326501\n",
      "Epoch: 7; Batch 300; Loss: 0.318007\n",
      "Epoch: 7; Batch 400; Loss: 0.346424\n",
      "Epoch: 7; Batch 500; Loss: 0.423897\n",
      "Epoch: 7; Batch 600; Loss: 0.390864\n",
      "Epoch: 8; Batch 100; Loss: 0.320052\n",
      "Epoch: 8; Batch 200; Loss: 0.399715\n",
      "Epoch: 8; Batch 300; Loss: 0.236497\n",
      "Epoch: 8; Batch 400; Loss: 0.316463\n",
      "Epoch: 8; Batch 500; Loss: 0.452773\n",
      "Epoch: 8; Batch 600; Loss: 0.273107\n",
      "Epoch: 9; Batch 100; Loss: 0.293153\n",
      "Epoch: 9; Batch 200; Loss: 0.226210\n",
      "Epoch: 9; Batch 300; Loss: 0.202463\n",
      "Epoch: 9; Batch 400; Loss: 0.296922\n",
      "Epoch: 9; Batch 500; Loss: 0.305041\n",
      "Epoch: 9; Batch 600; Loss: 0.348405\n",
      "Epoch: 10; Batch 100; Loss: 0.344109\n",
      "Epoch: 10; Batch 200; Loss: 0.286845\n",
      "Epoch: 10; Batch 300; Loss: 0.262044\n",
      "Epoch: 10; Batch 400; Loss: 0.295667\n",
      "Epoch: 10; Batch 500; Loss: 0.371402\n",
      "Epoch: 10; Batch 600; Loss: 0.359142\n",
      "Epoch: 11; Batch 100; Loss: 0.294819\n",
      "Epoch: 11; Batch 200; Loss: 0.305719\n",
      "Epoch: 11; Batch 300; Loss: 0.163066\n",
      "Epoch: 11; Batch 400; Loss: 0.245787\n",
      "Epoch: 11; Batch 500; Loss: 0.281740\n",
      "Epoch: 11; Batch 600; Loss: 0.276333\n",
      "Epoch: 12; Batch 100; Loss: 0.349648\n",
      "Epoch: 12; Batch 200; Loss: 0.351941\n",
      "Epoch: 12; Batch 300; Loss: 0.352166\n",
      "Epoch: 12; Batch 400; Loss: 0.271484\n",
      "Epoch: 12; Batch 500; Loss: 0.364350\n",
      "Epoch: 12; Batch 600; Loss: 0.420533\n",
      "Epoch: 13; Batch 100; Loss: 0.240329\n",
      "Epoch: 13; Batch 200; Loss: 0.324418\n",
      "Epoch: 13; Batch 300; Loss: 0.346842\n",
      "Epoch: 13; Batch 400; Loss: 0.382462\n",
      "Epoch: 13; Batch 500; Loss: 0.264406\n",
      "Epoch: 13; Batch 600; Loss: 0.310091\n",
      "Epoch: 14; Batch 100; Loss: 0.230696\n",
      "Epoch: 14; Batch 200; Loss: 0.412099\n",
      "Epoch: 14; Batch 300; Loss: 0.321631\n",
      "Epoch: 14; Batch 400; Loss: 0.199406\n",
      "Epoch: 14; Batch 500; Loss: 0.300925\n",
      "Epoch: 14; Batch 600; Loss: 0.274584\n",
      "Epoch: 15; Batch 100; Loss: 0.284748\n",
      "Epoch: 15; Batch 200; Loss: 0.268074\n",
      "Epoch: 15; Batch 300; Loss: 0.158816\n",
      "Epoch: 15; Batch 400; Loss: 0.237642\n",
      "Epoch: 15; Batch 500; Loss: 0.205464\n",
      "Epoch: 15; Batch 600; Loss: 0.342708\n",
      "Epoch: 16; Batch 100; Loss: 0.278530\n",
      "Epoch: 16; Batch 200; Loss: 0.222577\n",
      "Epoch: 16; Batch 300; Loss: 0.137565\n",
      "Epoch: 16; Batch 400; Loss: 0.195411\n",
      "Epoch: 16; Batch 500; Loss: 0.374841\n",
      "Epoch: 16; Batch 600; Loss: 0.283086\n",
      "Epoch: 17; Batch 100; Loss: 0.334784\n",
      "Epoch: 17; Batch 200; Loss: 0.222807\n",
      "Epoch: 17; Batch 300; Loss: 0.229298\n",
      "Epoch: 17; Batch 400; Loss: 0.311158\n",
      "Epoch: 17; Batch 500; Loss: 0.196632\n",
      "Epoch: 17; Batch 600; Loss: 0.212943\n",
      "Epoch: 18; Batch 100; Loss: 0.336671\n",
      "Epoch: 18; Batch 200; Loss: 0.185513\n",
      "Epoch: 18; Batch 300; Loss: 0.199848\n",
      "Epoch: 18; Batch 400; Loss: 0.393271\n",
      "Epoch: 18; Batch 500; Loss: 0.290595\n",
      "Epoch: 18; Batch 600; Loss: 0.302698\n",
      "Epoch: 19; Batch 100; Loss: 0.159360\n",
      "Epoch: 19; Batch 200; Loss: 0.240552\n",
      "Epoch: 19; Batch 300; Loss: 0.200192\n",
      "Epoch: 19; Batch 400; Loss: 0.289556\n",
      "Epoch: 19; Batch 500; Loss: 0.208361\n",
      "Epoch: 19; Batch 600; Loss: 0.185464\n",
      "Epoch: 20; Batch 100; Loss: 0.264784\n",
      "Epoch: 20; Batch 200; Loss: 0.154731\n",
      "Epoch: 20; Batch 300; Loss: 0.240149\n",
      "Epoch: 20; Batch 400; Loss: 0.300989\n",
      "Epoch: 20; Batch 500; Loss: 0.333881\n",
      "Epoch: 20; Batch 600; Loss: 0.221031\n",
      "Epoch: 21; Batch 100; Loss: 0.287084\n",
      "Epoch: 21; Batch 200; Loss: 0.352411\n",
      "Epoch: 21; Batch 300; Loss: 0.280168\n",
      "Epoch: 21; Batch 400; Loss: 0.208888\n",
      "Epoch: 21; Batch 500; Loss: 0.233016\n",
      "Epoch: 21; Batch 600; Loss: 0.245704\n",
      "Epoch: 22; Batch 100; Loss: 0.220662\n",
      "Epoch: 22; Batch 200; Loss: 0.156586\n",
      "Epoch: 22; Batch 300; Loss: 0.295607\n",
      "Epoch: 22; Batch 400; Loss: 0.321843\n",
      "Epoch: 22; Batch 500; Loss: 0.228088\n",
      "Epoch: 22; Batch 600; Loss: 0.207325\n",
      "Epoch: 23; Batch 100; Loss: 0.241008\n",
      "Epoch: 23; Batch 200; Loss: 0.287687\n",
      "Epoch: 23; Batch 300; Loss: 0.200243\n",
      "Epoch: 23; Batch 400; Loss: 0.243099\n",
      "Epoch: 23; Batch 500; Loss: 0.210563\n",
      "Epoch: 23; Batch 600; Loss: 0.301957\n",
      "Epoch: 24; Batch 100; Loss: 0.120565\n",
      "Epoch: 24; Batch 200; Loss: 0.249068\n",
      "Epoch: 24; Batch 300; Loss: 0.290935\n",
      "Epoch: 24; Batch 400; Loss: 0.174529\n",
      "Epoch: 24; Batch 500; Loss: 0.187597\n",
      "Epoch: 24; Batch 600; Loss: 0.306927\n",
      "Epoch: 25; Batch 100; Loss: 0.282331\n",
      "Epoch: 25; Batch 200; Loss: 0.112240\n",
      "Epoch: 25; Batch 300; Loss: 0.163614\n",
      "Epoch: 25; Batch 400; Loss: 0.177873\n",
      "Epoch: 25; Batch 500; Loss: 0.343600\n",
      "Epoch: 25; Batch 600; Loss: 0.216893\n",
      "Epoch: 26; Batch 100; Loss: 0.158486\n",
      "Epoch: 26; Batch 200; Loss: 0.186207\n",
      "Epoch: 26; Batch 300; Loss: 0.216735\n",
      "Epoch: 26; Batch 400; Loss: 0.153299\n",
      "Epoch: 26; Batch 500; Loss: 0.246982\n",
      "Epoch: 26; Batch 600; Loss: 0.193007\n",
      "Epoch: 27; Batch 100; Loss: 0.250162\n",
      "Epoch: 27; Batch 200; Loss: 0.233389\n",
      "Epoch: 27; Batch 300; Loss: 0.270445\n",
      "Epoch: 27; Batch 400; Loss: 0.252397\n",
      "Epoch: 27; Batch 500; Loss: 0.278783\n",
      "Epoch: 27; Batch 600; Loss: 0.339373\n",
      "Epoch: 28; Batch 100; Loss: 0.248368\n",
      "Epoch: 28; Batch 200; Loss: 0.290104\n",
      "Epoch: 28; Batch 300; Loss: 0.299915\n",
      "Epoch: 28; Batch 400; Loss: 0.154988\n",
      "Epoch: 28; Batch 500; Loss: 0.172182\n",
      "Epoch: 28; Batch 600; Loss: 0.276121\n",
      "Epoch: 29; Batch 100; Loss: 0.169550\n",
      "Epoch: 29; Batch 200; Loss: 0.096711\n",
      "Epoch: 29; Batch 300; Loss: 0.301185\n",
      "Epoch: 29; Batch 400; Loss: 0.251310\n",
      "Epoch: 29; Batch 500; Loss: 0.176777\n",
      "Epoch: 29; Batch 600; Loss: 0.124874\n",
      "Epoch: 30; Batch 100; Loss: 0.160293\n",
      "Epoch: 30; Batch 200; Loss: 0.195057\n",
      "Epoch: 30; Batch 300; Loss: 0.168571\n",
      "Epoch: 30; Batch 400; Loss: 0.294794\n",
      "Epoch: 30; Batch 500; Loss: 0.251548\n",
      "Epoch: 30; Batch 600; Loss: 0.137742\n",
      "Epoch: 31; Batch 100; Loss: 0.233855\n",
      "Epoch: 31; Batch 200; Loss: 0.122567\n",
      "Epoch: 31; Batch 300; Loss: 0.251845\n",
      "Epoch: 31; Batch 400; Loss: 0.166971\n",
      "Epoch: 31; Batch 500; Loss: 0.198056\n",
      "Epoch: 31; Batch 600; Loss: 0.183906\n",
      "Epoch: 32; Batch 100; Loss: 0.263097\n",
      "Epoch: 32; Batch 200; Loss: 0.235733\n",
      "Epoch: 32; Batch 300; Loss: 0.116841\n",
      "Epoch: 32; Batch 400; Loss: 0.346637\n",
      "Epoch: 32; Batch 500; Loss: 0.126242\n",
      "Epoch: 32; Batch 600; Loss: 0.292178\n",
      "Epoch: 33; Batch 100; Loss: 0.273914\n",
      "Epoch: 33; Batch 200; Loss: 0.194816\n",
      "Epoch: 33; Batch 300; Loss: 0.145975\n",
      "Epoch: 33; Batch 400; Loss: 0.204011\n",
      "Epoch: 33; Batch 500; Loss: 0.219024\n",
      "Epoch: 33; Batch 600; Loss: 0.247023\n",
      "Epoch: 34; Batch 100; Loss: 0.266131\n",
      "Epoch: 34; Batch 200; Loss: 0.260184\n",
      "Epoch: 34; Batch 300; Loss: 0.141787\n",
      "Epoch: 34; Batch 400; Loss: 0.191295\n",
      "Epoch: 34; Batch 500; Loss: 0.160464\n",
      "Epoch: 34; Batch 600; Loss: 0.127322\n",
      "Epoch: 35; Batch 100; Loss: 0.112578\n",
      "Epoch: 35; Batch 200; Loss: 0.261628\n",
      "Epoch: 35; Batch 300; Loss: 0.198770\n",
      "Epoch: 35; Batch 400; Loss: 0.226399\n",
      "Epoch: 35; Batch 500; Loss: 0.118978\n",
      "Epoch: 35; Batch 600; Loss: 0.133661\n",
      "Epoch: 36; Batch 100; Loss: 0.305930\n",
      "Epoch: 36; Batch 200; Loss: 0.292419\n",
      "Epoch: 36; Batch 300; Loss: 0.112822\n",
      "Epoch: 36; Batch 400; Loss: 0.172937\n",
      "Epoch: 36; Batch 500; Loss: 0.139896\n",
      "Epoch: 36; Batch 600; Loss: 0.099853\n",
      "Epoch: 37; Batch 100; Loss: 0.148325\n",
      "Epoch: 37; Batch 200; Loss: 0.101574\n",
      "Epoch: 37; Batch 300; Loss: 0.191067\n",
      "Epoch: 37; Batch 400; Loss: 0.313179\n",
      "Epoch: 37; Batch 500; Loss: 0.223587\n",
      "Epoch: 37; Batch 600; Loss: 0.091368\n",
      "Epoch: 38; Batch 100; Loss: 0.208742\n",
      "Epoch: 38; Batch 200; Loss: 0.181162\n",
      "Epoch: 38; Batch 300; Loss: 0.229233\n",
      "Epoch: 38; Batch 400; Loss: 0.135865\n",
      "Epoch: 38; Batch 500; Loss: 0.181762\n",
      "Epoch: 38; Batch 600; Loss: 0.136747\n",
      "Epoch: 39; Batch 100; Loss: 0.166180\n",
      "Epoch: 39; Batch 200; Loss: 0.280283\n",
      "Epoch: 39; Batch 300; Loss: 0.395570\n",
      "Epoch: 39; Batch 400; Loss: 0.164936\n",
      "Epoch: 39; Batch 500; Loss: 0.221464\n",
      "Epoch: 39; Batch 600; Loss: 0.099785\n",
      "Epoch: 40; Batch 100; Loss: 0.137131\n",
      "Epoch: 40; Batch 200; Loss: 0.139691\n",
      "Epoch: 40; Batch 300; Loss: 0.230667\n",
      "Epoch: 40; Batch 400; Loss: 0.226063\n",
      "Epoch: 40; Batch 500; Loss: 0.178634\n",
      "Epoch: 40; Batch 600; Loss: 0.129679\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs, model, train_dataloader, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 5, 2, 0, 5, 8, 7, 1, 1, 4, 5, 7, 8, 7, 9, 5, 0, 1, 3, 3, 8, 4, 6, 3,\n",
       "        9, 2, 5, 9, 7, 2, 0, 4, 4, 9, 1, 8, 1, 5, 6, 2, 5, 4, 4, 1, 3, 0, 1, 8,\n",
       "        1, 7, 1, 4, 0, 1, 3, 2, 0, 9, 4, 6, 4, 3, 8, 3, 4, 1, 3, 1, 2, 5, 8, 9,\n",
       "        3, 4, 7, 0, 1, 6, 7, 1, 0, 8, 6, 2, 5, 4, 5, 2, 3, 7, 1, 8, 3, 5, 0, 0,\n",
       "        3, 1, 6, 8])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images, test_labels = next(iter(test_dataloader))\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = model(test_images.view(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 5, 2, 0, 5, 6, 9, 1, 1, 4, 5, 7, 8, 7, 9, 5, 0, 1, 3, 3, 8, 4, 6, 3,\n",
       "        9, 2, 5, 7, 7, 2, 0, 4, 4, 9, 1, 8, 3, 5, 4, 2, 5, 4, 4, 1, 3, 0, 1, 8,\n",
       "        1, 7, 1, 6, 0, 1, 3, 2, 0, 9, 4, 2, 4, 3, 8, 3, 4, 1, 4, 1, 2, 5, 8, 9,\n",
       "        3, 6, 7, 6, 1, 0, 7, 1, 0, 8, 0, 2, 5, 4, 5, 2, 3, 7, 1, 8, 3, 5, 0, 0,\n",
       "        3, 1, 6, 8])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = torch.max(test_outputs, 1)[1]\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = [1 for i in range(100) if predicted[i] == test_labels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precentage_correct = sum(correct)/100\n",
    "precentage_correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
